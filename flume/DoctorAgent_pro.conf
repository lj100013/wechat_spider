#Name the compents on this agent
a1.sources = s1
a1.channels = c1 c2 c3 c4 c5 c6 c7 c8 c9 c10
a1.sinks = k1 k2 k3 k4 k5 k6 k7 k8 k9 k10


#=======================Configure kafka source================================ 
a1.sources.s1.type = org.apache.flume.source.kafka.KafkaSource

#要获取的Kafka的topic列表
a1.sources.s1.kafka.topics = pre_doctor_register_R2P4,pre_doctor_login_R2P4,pre_doctor_certify_R2P4,pre_doctor_check_R2P4,pre_doctor_change_R2P4

a1.sources.s1.kafka.consumer.group.id = flume2

#kafka集群中broker的列表
a1.sources.s1.kafka.bootstrap.servers = 120.79.48.155:9092,120.79.203.87:9092,120.79.232.226:9092

#在一个批次中写入channel中的最大消息量，即达到该数量，会刷写一次数据到channel
a1.sources.s1.batchSize = 1000

#在一个批次中写入channel中的最大时间间隔
a1.sources.s1.batchDurationMillis = 2000

# 将检索到的topic名，写入到一个Header，由topicHeader配置
a1.sources.s1.setTopicHeader = true

#定义header的名称，用于存储从Kafka获取到的topic名
a1.sources.s1.topicHeader = topic

#当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费
a1.sources.s1.kafka.consumer.auto.offset.reset = earliest
#=======================Configure kafka source================================ 



#=======================Configure interceptors================================ 
#给s1设置两个个拦截器，命名为i1 i2 i3
a1.sources.s1.interceptors = i1 i2

#设置i1拦截器的类型为 : 自定义拦截器
a1.sources.s1.interceptors.i1.type = com.dachen.bigdata.datax.flume.DoctorInterceptor_pro$Builder

#i2为时间拦截器
a1.sources.s1.interceptors.i2.type = timestamp
#preserveExisting参数如果为true时，如果headers中有key为timestamp的属性，就不替换其值了，如果preserverExisting参数为false，headers中没有key为timestamp的属性就添加上，如果有就更新其值
a1.sources.s1.interceptors.i2.preserveExisting = true

#=======================Configure interceptors================================ 



#=======================Configure selector================================
#s1对应的channel
a1.sources.s1.channels = c1 c2 c3 c4 c5 c6 c7 c8 c9 c10
#定义s1的channel选择器类型为：multiplexing
a1.sources.s1.selector.type = multiplexing
#定义channel选择器选择的依据是key名为topicSource的headers键值对
a1.sources.s1.selector.header = topicSource

a1.sources.s1.selector.mapping.pre_doctor_register_R2P4 = c1 c2
a1.sources.s1.selector.mapping.pre_doctor_login_R2P4 = c3 c4
a1.sources.s1.selector.mapping.pre_doctor_certify_R2P4 = c5 c6
a1.sources.s1.selector.mapping.pre_doctor_check_R2P4 = c7 c8
a1.sources.s1.selector.mapping.pre_doctor_change_R2P4 = c9 c10
#=======================Configure selector================================


#=======================Configure memory channel================================ 
a1.channels.c1.type = memory
#存储在channel中事件的最大量
a1.channels.c1.capacity = 5000
#Channel每次提交的Event数量
a1.channels.c1.transactionCapacity = 5000

a1.channels.c3.type = memory
a1.channels.c3.capacity = 5000
a1.channels.c3.transactionCapacity = 5000

a1.channels.c5.type = memory
a1.channels.c5.capacity = 5000
a1.channels.c5.transactionCapacity = 5000

a1.channels.c7.type = memory
a1.channels.c7.capacity = 5000
a1.channels.c7.transactionCapacity = 5000

a1.channels.c9.type = memory
a1.channels.c9.capacity = 5000
a1.channels.c9.transactionCapacity = 5000
#=======================Configure memory channel================================ 

#=======================Configure file channel================================ 
a1.channels.c2.type = file
#检查点文件所存储的目录
a1.channels.c2.checkpointDir = /data/flume/pro/doctor_agent_c2/checkpoint
#备份检测点如果为true,backupCheckpointDir必须设置，
#a1.channels.c2.useDualCheckpoints = false
#备份检测点的备份到所在的目录，不要与数据检测点或者目录重复
#a1.channels.c2.backupCheckpointDir = /data/flume/doctor_agent_c2/backup
#数据存储所在目录设置
a1.channels.c2.dataDirs = /data/flume/pro/doctor_agent_c2/data
#The maximum size of transaction supported by the channel
a1.channels.c2.transactionCapacity = 100000
#Maximum capacity of the channel
a1.channels.c2.capacity = 500000
#检测点之间的时间值设置（单位微秒）
a1.channels.c2.checkpointInterval = 60000
#一个存放操作的等待时间值（秒）设置
a1.channels.c2.keep-alive = 5
#一个单一日志的最大值设置（以字节为单位）
a1.channels.c2.maxFileSize = 5368709120

a1.channels.c4.type = file
a1.channels.c4.checkpointDir = /data/flume/pro/doctor_agent_c4/checkpoint
a1.channels.c4.useDualCheckpoints = false
#a1.channels.c4.backupCheckpointDir = /data/flume/doctor_agent_c4/backup
a1.channels.c4.dataDirs = /data/flume/pro/doctor_agent_c4/data
a1.channels.c4.transactionCapacity = 100000
a1.channels.c4.capacity = 500000
a1.channels.c4.checkpointInterval = 60000
a1.channels.c4.keep-alive = 5
a1.channels.c4.maxFileSize = 5368709120

a1.channels.c6.type = file
a1.channels.c6.checkpointDir = /data/flume/pro/doctor_agent_c6/checkpoint
a1.channels.c6.useDualCheckpoints = false
#a1.channels.c6.backupCheckpointDir = /data/flume/doctor_agent_c6/backup
a1.channels.c6.dataDirs = /data/flume/pro/doctor_agent_c6/data
a1.channels.c6.transactionCapacity = 100000
a1.channels.c6.capacity = 500000
a1.channels.c6.checkpointInterval = 60000
a1.channels.c6.keep-alive = 5
a1.channels.c6.maxFileSize = 5368709120

a1.channels.c8.type = file
a1.channels.c8.checkpointDir = /data/flume/pro/doctor_agent_c8/checkpoint
a1.channels.c8.useDualCheckpoints = false
#a1.channels.c8.backupCheckpointDir = /data/flume/doctor_agent_c8/backup
a1.channels.c8.dataDirs = /data/flume/pro/doctor_agent_c8/data
a1.channels.c8.transactionCapacity = 100000
a1.channels.c8.capacity = 500000
a1.channels.c8.checkpointInterval = 60000
a1.channels.c8.keep-alive = 5
a1.channels.c8.maxFileSize = 5368709120

a1.channels.c10.type = file
a1.channels.c10.checkpointDir = /data/flume/pro/doctor_agent_c10/checkpoint
a1.channels.c10.useDualCheckpoints = false
#a1.channels.c10.backupCheckpointDir = /data/flume/doctor_agent_c10/backup
a1.channels.c10.dataDirs = /data/flume/pro/doctor_agent_c10/data
a1.channels.c10.transactionCapacity = 100000
a1.channels.c10.capacity = 500000
a1.channels.c10.checkpointInterval = 60000
a1.channels.c10.keep-alive = 5
a1.channels.c10.maxFileSize = 5368709120
#=======================Configure file channel================================ 



#=======================Configure kafka sink================================ 
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.channel = c1
a1.sinks.k1.kafka.topic = pro_doctor_register_R2P4
a1.sinks.k1.kafka.bootstrap.servers = ns:9092,ds-02:9092,ds-03:9092,ds-04:9092
a1.sinks.k1.kafka.flumeBatchSize = 20
a1.sinks.k1.kafka.producer.acks = 1
a1.sinks.k1.kafka.producer.linger.ms = 1
a1.sinks.k1.kafka.producer.compression.type = snappy

a1.sinks.k3.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k3.channel = c3
a1.sinks.k3.kafka.topic = pro_doctor_login_R2P4
a1.sinks.k3.kafka.bootstrap.servers = ns:9092,ds-02:9092,ds-03:9092,ds-04:9092
a1.sinks.k3.kafka.flumeBatchSize = 20
a1.sinks.k3.kafka.producer.acks = 1
a1.sinks.k3.kafka.producer.linger.ms = 1
a1.sinks.k3.kafka.producer.compression.type = snappy

a1.sinks.k5.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k5.channel = c5
a1.sinks.k5.kafka.topic = pro_doctor_certify_R2P4
a1.sinks.k5.kafka.bootstrap.servers = ns:9092,ds-02:9092,ds-03:9092,ds-04:9092
a1.sinks.k5.kafka.flumeBatchSize = 20
a1.sinks.k5.kafka.producer.acks = 1
a1.sinks.k5.kafka.producer.linger.ms = 1
a1.sinks.k5.kafka.producer.compression.type = snappy

a1.sinks.k7.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k7.channel = c7
a1.sinks.k7.kafka.topic = pro_doctor_check_R2P4
a1.sinks.k7.kafka.bootstrap.servers = ns:9092,ds-02:9092,ds-03:9092,ds-04:9092
a1.sinks.k7.kafka.flumeBatchSize = 20
a1.sinks.k7.kafka.producer.acks = 1
a1.sinks.k7.kafka.producer.linger.ms = 1
a1.sinks.k7.kafka.producer.compression.type = snappy

a1.sinks.k9.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k9.channel = c9
a1.sinks.k9.kafka.topic = pro_doctor_change_R2P4
a1.sinks.k9.kafka.bootstrap.servers = ns:9092,ds-02:9092,ds-03:9092,ds-04:9092
a1.sinks.k9.kafka.flumeBatchSize = 20
a1.sinks.k9.kafka.producer.acks = 1
a1.sinks.k9.kafka.producer.linger.ms = 1
a1.sinks.k9.kafka.producer.compression.type = snappy

#=======================Configure kafka sink================================ 



#=======================Configure hdfs sink================================ 
#此处有个问题？---》1、要实现1天生成一个文件，如何实现？有按照时间、文件大小、event数目来进行roll，此处选择当文件大小为128M时，生成文件，要是一天内文件大小都达不到128M会出现什么情况？
a1.sinks.k2.type = hdfs
a1.sinks.k2.channel = c2
a1.sinks.k2.hdfs.path =  hdfs://nameservice1:8020/pro/data/collect/doctor/register/%Y-%m-%d/
a1.sinks.k2.hdfs.filePrefix = doctor_register-
a1.sinks.k2.hdfs.fileSuffix = .log
a1.sinks.k2.hdfs.inUsePrefix = _
a1.sinks.k2.hdfs.fileType = DataStream
#默认值：10,当events数据达到该数量时候，将临时文件滚动成目标文件；如果设置成0，则表示不根据events数据来滚动文件；
a1.sinks.k2.hdfs.rollCount = 0
#默认值：1024,当临时文件达到该大小（单位：bytes）时，滚动成目标文件；如果设置成0，则表示不根据临时文件大小来滚动文件；
a1.sinks.k2.hdfs.rollSize = 1073741824
#默认值：30,hdfs sink间隔多长将临时文件滚动成最终目标文件，单位：秒；如果设置成0，则表示不根据时间来滚动文件；注：滚动（roll）指的是，hdfs sink将临时文件重命名成最终目标文件，并新打开一个临时文件来写入数据；
a1.sinks.k2.hdfs.rollInterval = 3600
#默认值：100,每个批次刷新到HDFS上的events数量；
a1.sinks.k2.hdfs.batchSize = 100

a1.sinks.k4.type = hdfs
a1.sinks.k4.channel = c4
a1.sinks.k4.hdfs.path =  hdfs://nameservice1:8020/pro/data/collect/doctor/login/%Y-%m-%d/
a1.sinks.k4.hdfs.filePrefix = doctor_login-
a1.sinks.k4.hdfs.fileSuffix = .log
a1.sinks.k4.hdfs.inUsePrefix = _
a1.sinks.k4.hdfs.fileType = DataStream
a1.sinks.k4.hdfs.rollCount = 0
a1.sinks.k4.hdfs.rollSize = 1073741824
a1.sinks.k4.hdfs.rollInterval = 3600
a1.sinks.k4.hdfs.batchSize = 100

a1.sinks.k6.type = hdfs
a1.sinks.k6.channel = c6
a1.sinks.k6.hdfs.path =  hdfs://nameservice1:8020/pro/data/collect/doctor/certify/%Y-%m-%d/
a1.sinks.k6.hdfs.filePrefix = doctor_certify-
a1.sinks.k6.hdfs.fileSuffix = .log
a1.sinks.k6.hdfs.inUsePrefix = _
a1.sinks.k6.hdfs.fileType = DataStream
a1.sinks.k6.hdfs.rollCount = 0
a1.sinks.k6.hdfs.rollSize = 1073741824
a1.sinks.k6.hdfs.rollInterval = 3600
a1.sinks.k6.hdfs.batchSize = 100

a1.sinks.k8.type = hdfs
a1.sinks.k8.channel = c8
a1.sinks.k8.hdfs.path =  hdfs://nameservice1:8020/pro/data/collect/doctor/check/%Y-%m-%d/
a1.sinks.k8.hdfs.filePrefix = doctor_check-
a1.sinks.k8.hdfs.fileSuffix = .log
a1.sinks.k8.hdfs.inUsePrefix = _
a1.sinks.k8.hdfs.fileType = DataStream
a1.sinks.k8.hdfs.rollCount = 0
a1.sinks.k8.hdfs.rollSize = 1073741824
a1.sinks.k8.hdfs.rollInterval = 3600
a1.sinks.k8.hdfs.batchSize = 100

a1.sinks.k10.type = hdfs
a1.sinks.k10.channel = c10
a1.sinks.k10.hdfs.path =  hdfs://nameservice1:8020/pro/data/collect/doctor/change/%Y-%m-%d/
a1.sinks.k10.hdfs.filePrefix = doctor_change-
a1.sinks.k10.hdfs.fileSuffix = .log
a1.sinks.k10.hdfs.inUsePrefix = _
a1.sinks.k10.hdfs.fileType = DataStream
a1.sinks.k10.hdfs.rollCount = 0
a1.sinks.k10.hdfs.rollSize = 1073741824
a1.sinks.k10.hdfs.rollInterval = 3600
a1.sinks.k10.hdfs.batchSize = 100
#=======================Configure hdfs sink================================ 




