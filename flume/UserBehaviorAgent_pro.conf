#Name the compents on this agent
a1.sources = s1
a1.channels = c1
a1.sinks = k1


#=======================Configure kafka source================================
a1.sources.s1.type = org.apache.flume.source.kafka.KafkaSource

#要获取的Kafka的topic列表
a1.sources.s1.kafka.topics = pre_user_behavior_R2P3

a1.sources.s1.kafka.consumer.group.id = flume3

#kafka集群中broker的列表
#a1.sources.s1.kafka.bootstrap.servers = ds-01:9092
a1.sources.s1.kafka.bootstrap.servers = 120.79.48.155:9092,120.79.203.87:9092,120.79.232.226:9092

#在一个批次中写入channel中的最大消息量，即达到该数量，会刷写一次数据到channel
a1.sources.s1.batchSize = 1000

#在一个批次中写入channel中的最大时间间隔
a1.sources.s1.batchDurationMillis = 2000

# 将检索到的topic名，写入到一个Header，由topicHeader配置
#a1.sources.s1.setTopicHeader = true

#定义header的名称，用于存储从Kafka获取到的topic名
#a1.sources.s1.topicHeader = topic

#当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费
a1.sources.s1.kafka.consumer.auto.offset.reset = earliest
#=======================Configure kafka source================================



#=======================Configure interceptors================================
#给s1设置两个个拦截器，命名为i1 i2
a1.sources.s1.interceptors = i1 i2

#设置i1拦截器的类型为 : 自定义拦截器
a1.sources.s1.interceptors.i1.type = com.dachen.bigdata.datax.flume.UserBehaviorInterceptor_pro$Builder

#i2为时间拦截器
a1.sources.s1.interceptors.i2.type = timestamp
#preserveExisting参数如果为true时，如果headers中有key为timestamp的属性，就不替换其值了，如果preserverExisting参数为false，headers中没有key为timestamp的属性就添加上，如果有就更新其值
a1.sources.s1.interceptors.i2.preserveExisting = true

#=======================Configure interceptors================================

a1.sources.s1.channels = c1


#=======================Configure file channel================================
a1.channels.c1.type = file
#检查点文件所存储的目录
a1.channels.c1.checkpointDir = /data/flume/pro/userbehavior_agent_c1/checkpoint
#备份检测点如果为true,backupCheckpointDir必须设置，
#a1.channels.c1.useDualCheckpoints = false
#备份检测点的备份到所在的目录，不要与数据检测点或者目录重复
#a1.channels.c1.backupCheckpointDir = /data/flume/circle_agent_c1/backup
#数据存储所在目录设置
a1.channels.c1.dataDirs = /data/flume/pro/userbehavior_agent_c1/data
#The maximum size of transaction supported by the channel
a1.channels.c1.transactionCapacity = 100000
#Maximum capacity of the channel
a1.channels.c1.capacity = 500000
#检测点之间的时间值设置（单位微秒）
a1.channels.c1.checkpointInterval = 60000
#一个存放操作的等待时间值（秒）设置
a1.channels.c1.keep-alive = 5
#一个单一日志的最大值设置（以字节为单位）
a1.channels.c1.maxFileSize = 5368709120

#=======================Configure file channel================================

#=======================Configure hdfs sink================================
#此处有个问题？---》1、要实现1天生成一个文件，如何实现？有按照时间、文件大小、event数目来进行roll，此处选择当文件大小为128M时，生成文件，要是一天内文件大小都达不到128M会出现什么情况？
a1.sinks.k1.type = hdfs
a1.sinks.k1.channel = c1
a1.sinks.k1.hdfs.path =  hdfs://nameservice1:8020/pro/data/collect/userbehavior/%Y-%m-%d/
a1.sinks.k1.hdfs.filePrefix = userbehavior-
a1.sinks.k1.hdfs.fileSuffix = .log
a1.sinks.k1.hdfs.inUsePrefix = _
a1.sinks.k1.hdfs.fileType = DataStream
#默认值：10,当events数据达到该数量时候，将临时文件滚动成目标文件；如果设置成0，则表示不根据events数据来滚动文件；
a1.sinks.k1.hdfs.rollCount = 0
#默认值：1024,当临时文件达到该大小（单位：bytes）时，滚动成目标文件；如果设置成0，则表示不根据临时文件大小来滚动文件；
a1.sinks.k1.hdfs.rollSize = 1073741824
#默认值：30,hdfs sink间隔多长将临时文件滚动成最终目标文件，单位：秒；如果设置成0，则表示不根据时间来滚动文件；注：滚动（roll）指的是，hdfs sink将临时文件重命名成最终目标文件，并新打开一个临时文件来写入数据；
a1.sinks.k1.hdfs.rollInterval = 3600
#默认值：100,每个批次刷新到HDFS上的events数量；
a1.sinks.k1.hdfs.batchSize = 100

#=======================Configure hdfs sink================================




