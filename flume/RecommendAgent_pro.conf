
#Name the compents on this agent
a1.sources = s1
a1.channels = c1 c2 c3 c4 c5 c6 c7 c8 c9 c10 c11 c12 c13 c14
a1.sinks = k1 k2 k3 k4 k5 k6 k7 k8 k9 k10 k11 k12 k13 k14


#=======================Configure kafka source================================ 
a1.sources.s1.type = org.apache.flume.source.kafka.KafkaSource

#要获取的Kafka的topic列表
a1.sources.s1.kafka.topics = pre_publish_faq_R2P3,pre_publish_question_R2P3,pre_publish_disease_R2P3,pre_publish_reward_R2P3,pre_publish_broadcast_R2P3,pre_publish_document_R2P3,pre_publish_course_R2P3

a1.sources.s1.kafka.consumer.group.id = flume5

#kafka集群中broker的列表
a1.sources.s1.kafka.bootstrap.servers = 120.79.48.155:9092,120.79.203.87:9092,120.79.232.226:9092

#在一个批次中写入channel中的最大消息量，即达到该数量，会刷写一次数据到channel
a1.sources.s1.batchSize = 5000

#在一个批次中写入channel中的最大时间间隔
a1.sources.s1.batchDurationMillis = 2000

# 将检索到的topic名，写入到一个Header，由topicHeader配置
a1.sources.s1.setTopicHeader = true

#定义header的名称，用于存储从Kafka获取到的topic名
a1.sources.s1.topicHeader = topic

#当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费
a1.sources.s1.kafka.consumer.auto.offset.reset = earliest
#=======================Configure kafka source================================ 



#=======================Configure interceptors================================ 
#给s1设置两个个拦截器，命名为i1 i2
a1.sources.s1.interceptors = i1 i2

#设置i1拦截器的类型为 : 自定义拦截器
a1.sources.s1.interceptors.i1.type = com.dachen.bigdata.datax.flume.RecommendInterceptor_pro$Builder

#i2为时间拦截器
a1.sources.s1.interceptors.i2.type = timestamp
#preserveExisting参数如果为true时，如果headers中有key为timestamp的属性，就不替换其值了，如果preserverExisting参数为false，headers中没有key为timestamp的属性就添加上，如果有就更新其值
a1.sources.s1.interceptors.i2.preserveExisting = true

#=======================Configure interceptors================================ 



#=======================Configure selector================================
#s1对应的channel
a1.sources.s1.channels = c1 c2 c3 c4 c5 c6 c7 c8 c9 c10 c11 c12 c13 c14
#定义s1的channel选择器类型为：multiplexing
a1.sources.s1.selector.type = multiplexing
#定义channel选择器选择的依据是key名为topicSource的headers键值对
a1.sources.s1.selector.header = topicSource

a1.sources.s1.selector.mapping.pre_publish_faq_R2P3 = c1 c2
a1.sources.s1.selector.mapping.pre_publish_question_R2P3 = c3 c4
a1.sources.s1.selector.mapping.pre_publish_disease_R2P3 = c5 c6
a1.sources.s1.selector.mapping.pre_publish_reward_R2P3 = c7 c8
a1.sources.s1.selector.mapping.pre_publish_broadcast_R2P3 = c9 c10
a1.sources.s1.selector.mapping.pre_publish_document_R2P3 = c11 c12
a1.sources.s1.selector.mapping.pre_publish_course_R2P3 = c13 c14
#=======================Configure selector================================


#=======================Configure memory channel================================ 
a1.channels.c1.type = memory
#存储在channel中事件的最大量
a1.channels.c1.capacity = 5000
#Channel每次提交的Event数量
a1.channels.c1.transactionCapacity = 5000

a1.channels.c3.type = memory
a1.channels.c3.capacity = 5000
a1.channels.c3.transactionCapacity = 5000

a1.channels.c5.type = memory
a1.channels.c5.capacity = 5000
a1.channels.c5.transactionCapacity = 5000

a1.channels.c7.type = memory
a1.channels.c7.capacity = 5000
a1.channels.c7.transactionCapacity = 5000

a1.channels.c9.type = memory
a1.channels.c9.capacity = 5000
a1.channels.c9.transactionCapacity = 5000

a1.channels.c11.type = memory
a1.channels.c11.capacity = 5000
a1.channels.c11.transactionCapacity = 5000

a1.channels.c13.type = memory
a1.channels.c13.capacity = 5000
a1.channels.c13.transactionCapacity = 5000

#=======================Configure memory channel================================ 

#=======================Configure file channel================================ 
a1.channels.c2.type = file
#检查点文件所存储的目录
a1.channels.c2.checkpointDir = /data/flume/pro/recommend_agent_c2/checkpoint
#备份检测点如果为true,backupCheckpointDir必须设置，
a1.channels.c2.useDualCheckpoints = false
#备份检测点的备份到所在的目录，不要与数据检测点或者目录重复
#a1.channels.c2.backupCheckpointDir = /data/flume/recommend_agent_c2/backup
#数据存储所在目录设置
a1.channels.c2.dataDirs = /data/flume/pro/recommend_agent_c2/data
#The maximum size of transaction supported by the channel
a1.channels.c2.transactionCapacity = 100000
#Maximum capacity of the channel
a1.channels.c2.capacity = 500000
#检测点之间的时间值设置（单位微秒）
a1.channels.c2.checkpointInterval = 60000
#一个存放操作的等待时间值（秒）设置
a1.channels.c2.keep-alive = 5
#一个单一日志的最大值设置（以字节为单位）
a1.channels.c2.maxFileSize = 5368709120


a1.channels.c4.type = file
a1.channels.c4.checkpointDir = /data/flume/pro/recommend_agent_c4/checkpoint
a1.channels.c4.useDualCheckpoints = false
a1.channels.c4.dataDirs = /data/flume/pro/recommend_agent_c4/data
a1.channels.c4.transactionCapacity = 100000
a1.channels.c4.capacity = 500000
a1.channels.c4.checkpointInterval = 60000
a1.channels.c4.keep-alive = 5
a1.channels.c4.maxFileSize = 5368709120

a1.channels.c6.type = file
a1.channels.c6.checkpointDir = /data/flume/pro/recommend_agent_c6/checkpoint
a1.channels.c6.useDualCheckpoints = false
a1.channels.c6.dataDirs = /data/flume/pro/recommend_agent_c6/data
a1.channels.c6.transactionCapacity = 100000
a1.channels.c6.capacity = 500000
a1.channels.c6.checkpointInterval = 60000
a1.channels.c6.keep-alive = 5
a1.channels.c6.maxFileSize = 5368709120


a1.channels.c8.type = file
a1.channels.c8.checkpointDir = /data/flume/pro/recommend_agent_c8/checkpoint
a1.channels.c8.useDualCheckpoints = false
a1.channels.c8.dataDirs = /data/flume/pro/recommend_agent_c8/data
a1.channels.c8.transactionCapacity = 100000
a1.channels.c8.capacity = 500000
a1.channels.c8.checkpointInterval = 60000
a1.channels.c8.keep-alive = 5
a1.channels.c8.maxFileSize = 5368709120


a1.channels.c10.type = file
a1.channels.c10.checkpointDir = /data/flume/pro/recommend_agent_c10/checkpoint
a1.channels.c10.useDualCheckpoints = false
a1.channels.c10.dataDirs = /data/flume/pro/recommend_agent_c10/data
a1.channels.c10.transactionCapacity = 100000
a1.channels.c10.capacity = 500000
a1.channels.c10.checkpointInterval = 60000
a1.channels.c10.keep-alive = 5
a1.channels.c10.maxFileSize = 5368709120


a1.channels.c12.type = file
a1.channels.c12.checkpointDir = /data/flume/pro/recommend_agent_c12/checkpoint
a1.channels.c12.useDualCheckpoints = false
a1.channels.c12.dataDirs = /data/flume/pro/recommend_agent_c12/data
a1.channels.c12.transactionCapacity = 100000
a1.channels.c12.capacity = 500000
a1.channels.c12.checkpointInterval = 60000
a1.channels.c12.keep-alive = 5
a1.channels.c12.maxFileSize = 5368709120


a1.channels.c14.type = file
a1.channels.c14.checkpointDir = /data/flume/pro/recommend_agent_c14/checkpoint
a1.channels.c14.useDualCheckpoints = false
a1.channels.c14.dataDirs = /data/flume/pro/recommend_agent_c14/data
a1.channels.c14.transactionCapacity = 100000
a1.channels.c14.capacity = 500000
a1.channels.c14.checkpointInterval = 60000
a1.channels.c14.keep-alive = 5
a1.channels.c14.maxFileSize = 5368709120
#=======================Configure file channel================================ 



#=======================Configure kafka sink================================ 
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.channel = c1
a1.sinks.k1.kafka.topic = pro_publish_faq_R2P3
a1.sinks.k1.kafka.bootstrap.servers = ns:9092,ds-02:9092,ds-03:9092,ds-04:9092
a1.sinks.k1.kafka.flumeBatchSize = 20
a1.sinks.k1.kafka.producer.acks = 1
a1.sinks.k1.kafka.producer.linger.ms = 1
a1.sinks.k1.kafka.producer.compression.type = snappy

a1.sinks.k3.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k3.channel = c3
a1.sinks.k3.kafka.topic = pro_publish_question_R2P3
a1.sinks.k3.kafka.bootstrap.servers = ns:9092,ds-02:9092,ds-03:9092,ds-04:9092
a1.sinks.k3.kafka.flumeBatchSize = 20
a1.sinks.k3.kafka.producer.acks = 1
a1.sinks.k3.kafka.producer.linger.ms = 1
a1.sinks.k3.kafka.producer.compression.type = snappy

a1.sinks.k5.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k5.channel = c5
a1.sinks.k5.kafka.topic = pro_publish_disease_R2P3
a1.sinks.k5.kafka.bootstrap.servers = ns:9092,ds-02:9092,ds-03:9092,ds-04:9092
a1.sinks.k5.kafka.flumeBatchSize = 20
a1.sinks.k5.kafka.producer.acks = 1
a1.sinks.k5.kafka.producer.linger.ms = 1
a1.sinks.k5.kafka.producer.compression.type = snappy

a1.sinks.k7.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k7.channel = c7
a1.sinks.k7.kafka.topic = pro_publish_reward_R2P3
a1.sinks.k7.kafka.bootstrap.servers = ns:9092,ds-02:9092,ds-03:9092,ds-04:9092
a1.sinks.k7.kafka.flumeBatchSize = 20
a1.sinks.k7.kafka.producer.acks = 1
a1.sinks.k7.kafka.producer.linger.ms = 1
a1.sinks.k7.kafka.producer.compression.type = snappy

a1.sinks.k9.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k9.channel = c9
a1.sinks.k9.kafka.topic = pro_publish_broadcast_R2P3
a1.sinks.k9.kafka.bootstrap.servers = ns:9092,ds-02:9092,ds-03:9092,ds-04:9092
a1.sinks.k9.kafka.flumeBatchSize = 20
a1.sinks.k9.kafka.producer.acks = 1
a1.sinks.k9.kafka.producer.linger.ms = 1
a1.sinks.k9.kafka.producer.compression.type = snappy

a1.sinks.k11.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k11.channel = c11
a1.sinks.k11.kafka.topic = pro_publish_document_R2P3
a1.sinks.k11.kafka.bootstrap.servers = ns:9092,ds-02:9092,ds-03:9092,ds-04:9092
a1.sinks.k11.kafka.flumeBatchSize = 20
a1.sinks.k11.kafka.producer.acks = 1
a1.sinks.k11.kafka.producer.linger.ms = 1
a1.sinks.k11.kafka.producer.compression.type = snappy


a1.sinks.k13.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k13.channel = c13
a1.sinks.k13.kafka.topic = pro_publish_course_R2P3
a1.sinks.k13.kafka.bootstrap.servers = ns:9092,ds-02:9092,ds-03:9092,ds-04:9092
a1.sinks.k13.kafka.flumeBatchSize = 20
a1.sinks.k13.kafka.producer.acks = 1
a1.sinks.k13.kafka.producer.linger.ms = 1
a1.sinks.k13.kafka.producer.compression.type = snappy

#=======================Configure kafka sink================================ 



#=======================Configure hdfs sink================================ 


#此处有个问题？---》1、要实现1天生成一个文件，如何实现？有按照时间、文件大小、event数目来进行roll，此处选择当文件大小为128M时，生成文件，要是一天内文件大小都达不到128M会出现什么情况？
a1.sinks.k2.type = hdfs
a1.sinks.k2.channel = c2
a1.sinks.k2.hdfs.path =  hdfs://nameservice1:8020/pro/data/collect/recommend/publishfaq/dt=%Y-%m-%d
a1.sinks.k2.hdfs.filePrefix = recommend-
a1.sinks.k2.hdfs.fileSuffix = .log
a1.sinks.k2.hdfs.inUsePrefix = _
a1.sinks.k2.hdfs.fileType = DataStream
#默认值：10,当events数据达到该数量时候，将临时文件滚动成目标文件；如果设置成0，则表示不根据events数据来滚动文件；
a1.sinks.k2.hdfs.rollCount = 0
#默认值：1024,当临时文件达到该大小（单位：bytes）时，滚动成目标文件；如果设置成0，则表示不根据临时文件大小来滚动文件；
a1.sinks.k2.hdfs.rollSize = 1073741824
#默认值：30,hdfs sink间隔多长将临时文件滚动成最终目标文件，单位：秒；如果设置成0，则表示不根据时间来滚动文件；注：滚动（roll）指的是，hdfs sink将临时文件重命名成最终目标文件，并新打开一个临时文件来写入数据；
a1.sinks.k2.hdfs.rollInterval = 3600
#默认值：100,每个批次刷新到HDFS上的events数量；
a1.sinks.k2.hdfs.batchSize = 100


a1.sinks.k4.type = hdfs
a1.sinks.k4.channel = c4
a1.sinks.k4.hdfs.path =  hdfs://nameservice1:8020/pro/data/collect/recommend/publishquestion/dt=%Y-%m-%d
a1.sinks.k4.hdfs.filePrefix = recommend-
a1.sinks.k4.hdfs.fileSuffix = .log
a1.sinks.k4.hdfs.inUsePrefix = _
a1.sinks.k4.hdfs.fileType = DataStream
a1.sinks.k4.hdfs.rollCount = 0
a1.sinks.k4.hdfs.rollSize = 1073741824
a1.sinks.k4.hdfs.rollInterval = 3600
a1.sinks.k4.hdfs.batchSize = 100


a1.sinks.k6.type = hdfs
a1.sinks.k6.channel = c6
a1.sinks.k6.hdfs.path =  hdfs://nameservice1:8020/pro/data/collect/recommend/publishdisease/dt=%Y-%m-%d
a1.sinks.k6.hdfs.filePrefix = recommend-
a1.sinks.k6.hdfs.fileSuffix = .log
a1.sinks.k6.hdfs.inUsePrefix = _
a1.sinks.k6.hdfs.fileType = DataStream
a1.sinks.k6.hdfs.rollCount = 0
a1.sinks.k6.hdfs.rollSize = 1073741824
a1.sinks.k6.hdfs.rollInterval = 3600
a1.sinks.k6.hdfs.batchSize = 100


a1.sinks.k8.type = hdfs
a1.sinks.k8.channel = c8
a1.sinks.k8.hdfs.path =  hdfs://nameservice1:8020/pro/data/collect/recommend/publishreward/dt=%Y-%m-%d
a1.sinks.k8.hdfs.filePrefix = recommend-
a1.sinks.k8.hdfs.fileSuffix = .log
a1.sinks.k8.hdfs.inUsePrefix = _
a1.sinks.k8.hdfs.fileType = DataStream
a1.sinks.k8.hdfs.rollCount = 0
a1.sinks.k8.hdfs.rollSize = 1073741824
a1.sinks.k8.hdfs.rollInterval = 3600
a1.sinks.k8.hdfs.batchSize = 100


a1.sinks.k10.type = hdfs
a1.sinks.k10.channel = c10
a1.sinks.k10.hdfs.path =  hdfs://nameservice1:8020/pro/data/collect/recommend/publishbroadcast/dt=%Y-%m-%d
a1.sinks.k10.hdfs.filePrefix = recommend-
a1.sinks.k10.hdfs.fileSuffix = .log
a1.sinks.k10.hdfs.inUsePrefix = _
a1.sinks.k10.hdfs.fileType = DataStream
a1.sinks.k10.hdfs.rollCount = 0
a1.sinks.k10.hdfs.rollSize = 1073741824
a1.sinks.k10.hdfs.rollInterval = 3600
a1.sinks.k10.hdfs.batchSize = 100

a1.sinks.k12.type = hdfs
a1.sinks.k12.channel = c12
a1.sinks.k12.hdfs.path =  hdfs://nameservice1:8020/pro/data/collect/recommend/publishdocument/dt=%Y-%m-%d
a1.sinks.k12.hdfs.filePrefix = recommend-
a1.sinks.k12.hdfs.fileSuffix = .log
a1.sinks.k12.hdfs.inUsePrefix = _
a1.sinks.k12.hdfs.fileType = DataStream
a1.sinks.k12.hdfs.rollCount = 0
a1.sinks.k12.hdfs.rollSize = 1073741824
a1.sinks.k12.hdfs.rollInterval = 3600
a1.sinks.k12.hdfs.batchSize = 100



a1.sinks.k14.type = hdfs
a1.sinks.k14.channel = c14
a1.sinks.k14.hdfs.path =  hdfs://nameservice1:8020/pro/data/collect/recommend/publishcourse/dt=%Y-%m-%d
a1.sinks.k14.hdfs.filePrefix = recommend-
a1.sinks.k14.hdfs.fileSuffix = .log
a1.sinks.k14.hdfs.inUsePrefix = _
a1.sinks.k14.hdfs.fileType = DataStream
a1.sinks.k14.hdfs.rollCount = 0
a1.sinks.k14.hdfs.rollSize = 1073741824
a1.sinks.k14.hdfs.rollInterval = 3600
a1.sinks.k14.hdfs.batchSize = 100

#=======================Configure hdfs sink================================ 



##=======================Configure hive sink================================ 
#
#a1.sinks.k2.type = hive
#a1.sinks.k2.channel = c2
##hive元存储的url
#a1.sinks.k2.hive.metastore = thrift://192.168.3.158:9083
##hive表库名
#a1.sinks.k2.hive.database = pro
##hive表表名
#a1.sinks.k2.hive.table = ods_publish_faq
##hive表分区，逗号分隔，%Y代表2018，&y代表18
#a1.sinks.k2.hive.partition = %Y-%m-%d
##此处自动创建分区必须关闭，否则会报错。使用手动构建分区
#a1.sinks.k2.autoCreatePartitions = false
##使用本地时间（而不是事件头的时间戳）
#a1.sinks.k2.useLocalTimeStamp = false
##a1.sinks.k2.round = true
##a1.sinks.k2.roundValue = 1
##a1.sinks.k2.roundUnit = minute
#a1.sinks.k2.serializer = DELIMITED
##切记切记，一定要记得转义
#a1.sinks.k2.serializer.delimiter = "\\001"
##a1.sinks.k2.serializer.serdeSeparator = "\\001"
##在Flume配置的Hive 列名必须都为小写字母。Hive表必须设置bucket并且 stored as orc。
#a1.sinks.k2.serializer.fieldnames = dstype,id,userid,circleid,dept,category,posttype,content,link,createtime,lastuploadtime,authername,autherdept,autherskill
#
#
#a1.sinks.k4.type = hive
#a1.sinks.k4.channel = c4
#a1.sinks.k4.hive.metastore = thrift://192.168.3.158:9083
#a1.sinks.k4.hive.database = pro
#a1.sinks.k4.hive.table = ods_publish_question
#a1.sinks.k4.hive.partition = %Y-%m-%d
#a1.sinks.k4.autoCreatePartitions = false
#a1.sinks.k4.useLocalTimeStamp = false
#a1.sinks.k4.serializer = DELIMITED
#a1.sinks.k4.serializer.delimiter = "\\001"
#a1.sinks.k4.serializer.fieldnames = dstype,id,userid,circleid,dept,category,type,content,answerid,paynum,isopen,createtime,lastuploadtime,authername,autherdept,autherskill
#
#
#a1.sinks.k6.type = hive
#a1.sinks.k6.channel = c6
#a1.sinks.k6.hive.metastore = thrift://192.168.3.158:9083
#a1.sinks.k6.hive.database = pro
#a1.sinks.k6.hive.table = ods_publish_disease
#a1.sinks.k6.hive.partition = %Y-%m-%d
#a1.sinks.k6.autoCreatePartitions = false
#a1.sinks.k6.useLocalTimeStamp = false
#a1.sinks.k6.serializer = DELIMITED
#a1.sinks.k6.serializer.delimiter = "\\001"
#a1.sinks.k6.serializer.fieldnames = dstype,id,userid,circleid,tag,title,content,createtime,lastuploadtime,authername,autherdept,autherskill
#
#
#a1.sinks.k8.type = hive
#a1.sinks.k8.channel = c8
#a1.sinks.k8.hive.metastore = thrift://192.168.3.158:9083
#a1.sinks.k8.hive.database = pro
#a1.sinks.k8.hive.table = ods_publish_reward
#a1.sinks.k8.hive.partition = %Y-%m-%d
#a1.sinks.k8.autoCreatePartitions = false
#a1.sinks.k8.useLocalTimeStamp = false
#a1.sinks.k8.serializer = DELIMITED
#a1.sinks.k8.serializer.delimiter = "\\001"
#a1.sinks.k8.serializer.fieldnames = dstype,id,userid,circleid,dept,category,posttype,content,scope,createtime,lastuploadtime,authername,autherdept,autherskill
#
#
#a1.sinks.k10.type = hive
#a1.sinks.k10.channel = c10
#a1.sinks.k10.hive.metastore = thrift://192.168.3.158:9083
#a1.sinks.k10.hive.database = pro
#a1.sinks.k10.hive.table = ods_publish_broadcast
#a1.sinks.k10.hive.partition = %Y-%m-%d
#a1.sinks.k10.autoCreatePartitions = false
#a1.sinks.k10.useLocalTimeStamp = false
#a1.sinks.k10.serializer = DELIMITED
#a1.sinks.k10.serializer.delimiter = "\\001"
#a1.sinks.k10.serializer.fieldnames = dstype,id,userid,circleid,speakerid,category,type,ispub,title,content,createtime,lastuploadtime,authername,autherdept,autherskill
#
#
#a1.sinks.k12.type = hive
#a1.sinks.k12.channel = c12
#a1.sinks.k12.hive.metastore = thrift://192.168.3.158:9083
#a1.sinks.k12.hive.database = pro
#a1.sinks.k12.hive.table = ods_publish_document
#a1.sinks.k12.hive.partition = %Y-%m-%d
#a1.sinks.k12.autoCreatePartitions = false
#a1.sinks.k12.useLocalTimeStamp = false
#a1.sinks.k12.serializer = DELIMITED
#a1.sinks.k12.serializer.delimiter = "\\001"
#a1.sinks.k12.serializer.fieldnames = dstype,id,userid,category,title,content,createtime,lastuploadtime
#
#=======================Configure hdfs sink================================ 
